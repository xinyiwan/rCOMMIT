{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = nib.streamlines.load('599671/all.tck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': '0010000000',\n",
       " 'datatype': 'Float32LE',\n",
       " 'voxel_sizes': '(1.25, 1.25, 1.25)',\n",
       " 'dimensions': '(145, 174, 145)',\n",
       " 'voxel_order': 'LAS',\n",
       " 'file': '. 145',\n",
       " 'magic_number': b'mrtrix tracks',\n",
       " 'endianness': '<',\n",
       " '_dtype': dtype('float32'),\n",
       " '_offset_data': 145,\n",
       " 'nb_streamlines': 10000000,\n",
       " 'voxel_to_rasmm': array([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.header['voxel_to_rasmm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_streamlines = 500000\n",
    "idx_list = np.random.choice(\n",
    "        np.arange(raw.header[\"nb_streamlines\"]),\n",
    "        size=num_streamlines,\n",
    "        replace=False,\n",
    "    ).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# json_idx = json.dump(idx_list)\n",
    "\n",
    "with open('idx.json', 'w') as f:\n",
    "    json.dump(idx_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets_info = {\n",
    "    \"size\": [500000],\n",
    "    \"idx\": idx_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "sub_file_path = '/Users/xinyi/Documents/GitHub/rCOMMIT'\n",
    "with io.open(os.path.join(sub_file_path, 'idx.json'), 'w') as f:\n",
    "            json.dump(subsets_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_new = nib.streamlines.Tractogram(\n",
    "    streamlines=raw.streamlines[idx_list],\n",
    "    affine_to_rasmm=raw.header['voxel_to_rasmm']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nib.streamlines.save(\n",
    "    t_new,\n",
    "    \"new_%s.tck\" %num_streamlines,\n",
    "    header=raw.header\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m\n",
      "-> Creating the dictionary from tractogram:\u001b[0m\n",
      "\u001b[0;32m\n",
      "   * Configuration:\u001b[0m\n",
      "\t- Segment position = COMPUTE INTERSECTIONS\n",
      "\t- Coordinates shift in X = 0.500 (voxel-size units)\n",
      "\t- Coordinates shift in Y = 0.500 (voxel-size units)\n",
      "\t- Coordinates shift in Z = 0.500 (voxel-size units)\n",
      "\t- Min segment len  = 0.001 mm\n",
      "\t- Min streamline len    = 0.00 mm\n",
      "\t- Max streamline len    = 250.00 mm\n",
      "\t- Do not blur streamlines\n",
      "\t- Output written to \"test_subsets/sub_500000/COMMIT\"\n",
      "\u001b[0;32m\n",
      "   * Loading data:\u001b[0m\n",
      "\t- Tractogram\n",
      "\t\t- geometry taken from \"/Users/xinyi/Documents/GitHub/rCOMMIT/599671/peaks.nii.gz\"\n",
      "\t\t- 145 x 174 x 145\n",
      "\t\t- 1.2500 x 1.2500 x 1.2500\n",
      "\t\t- 500000 streamlines\n",
      "\t- No mask specified to filter IC compartments\n",
      "\t- EC orientations\n",
      "\t\t- 145 x 174 x 145 x 9\n",
      "\t\t- 1.2500 x 1.2500 x 1.2500\n",
      "\t\t- ignoring peaks < 0.10 * MaxPeak\n",
      "\t\t- using affine matrix\n",
      "\t\t- flipping axes : [ x=False, y=False, z=False ]\n"
     ]
    }
   ],
   "source": [
    "from func.rc_run_commit_once import run_commit\n",
    "\n",
    "run_commit('test_subsets/sub_500000/new_500000.tck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test py\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import io\n",
    "from func.rc_generate_subsets import generate_subsets\n",
    "\n",
    "raw_file_path = '/Users/xinyi/Documents/GitHub/rCOMMIT/599671'\n",
    "subset_sizes = [100000, 50000, 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(raw_file_path):\n",
    "    rawfile = glob.glob(os.path.join(raw_file_path, \"*.tck\"), recursive=True)\n",
    "if rawfile[0]:\n",
    "    rawfile = rawfile[0]\n",
    "\n",
    "raw = nib.streamlines.load(rawfile)\n",
    "for i in range(len(subset_sizes)):\n",
    "\n",
    "    ## Make files for each subset\n",
    "    if not os.path.exists(os.path.join(raw_file_path,\"sub_%s\" %subset_sizes[i])):\n",
    "        sub_file_path = os.path.join(raw_file_path,\"sub_%s\" %subset_sizes[i])\n",
    "\n",
    "        os.makedirs(sub_file_path)\n",
    "    ## Generate random index for subset\n",
    "    num_streamlines = subset_sizes[i]\n",
    "    idx_list = np.random.choice(\n",
    "        np.arange(raw.header[\"nb_streamlines\"]),\n",
    "        size=num_streamlines,\n",
    "        replace=False,\n",
    "    ).tolist()\n",
    "\n",
    "    ## save it to the subsets path\n",
    "    subsets_info = {\"size\": [subset_sizes[i]], \"idx\": idx_list}\n",
    "    with io.open(os.path.join(sub_file_path, 'idx.json'), 'w') as f:\n",
    "        json.dump(subsets_info, f)\n",
    "\n",
    "    ## Generate subsets\n",
    "    t_new = nib.streamlines.Tractogram(\n",
    "        streamlines=raw.streamlines[idx_list],\n",
    "        affine_to_rasmm=raw.header['voxel_to_rasmm']\n",
    "    )\n",
    "    nib.streamlines.save(\n",
    "        t_new,\n",
    "        os.path.join(sub_file_path,\"new_%s.tck\" %num_streamlines),\n",
    "        header=raw.header\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ['1','2','f']\n",
    "with open(os.path.join('/Users/xinyi/Documents/GitHub/rCOMMIT/','paths.txt'), \"w\") as output:\n",
    "            for i in list:\n",
    "                output.write(str(i) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a17abb8c297f048084231843eaea2fdedfb5dcb77478369c4ee2132b9d4bbd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
