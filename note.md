* [X] Get test subsets
* [ ] Save index of subsets
* [ ] make a expriments for all subsets to run multiple times.
* [ ] Pick all the txt files and summerize the results
* [ ] Use two subjects and build a model & test --> performace
* [ ] Tool development
* [ ] Think deeper about a ramdom original size as input. What will the subsets look like?

  * [ ] A function to calculate how many times a subset should run COMMIT
  * [ ] Data structure of the input and output

raw data -> size

10mio  5

5mio 10

2.5mio 20

1.25mio 40

625k 80

500k 100

250k 200

---

10mio  5   ï¼ˆ5h)

5mio 10

500k 100

250k 200

---

Raw data --> give sizes of

romove bigger files: tck, npy, dict

---



size -- build a big df for all weights of idx

-- for each (idx&weight) build a df

-- concatenate each df to the big 10mio df


summarize info from the big df
